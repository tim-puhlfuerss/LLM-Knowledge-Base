# ü§ù Responsible AI

## [Foundation Model Transparency Index](https://crfm.stanford.edu/2024/05/21/fmti-may-2024.html)

- Index by Stanford researchers about transparency of well-known LLM providers
- [Pre-print](https://crfm.stanford.edu/fmti/paper.pdf) of v1.1 from May 2024 on Stanford website
- [Pre-print](https://doi.org/10.48550/arXiv.2310.12941) of v1.0 from October 2023 on arXiv
- [News article](https://the-decoder.de/stanford-studie-ki-firmen-legen-fortschritte-bei-transparenz-hin-aber-luft-nach-oben-bleibt/) by The Decoder

## Guidelines

### AI Guidelines by European Union

- Generative AI and the EUDPR. First EDPS Orientations for ensuring data protection compliance when using Generative AI systems (June 2024, by the European Data Protection Supervisor)
  - [Guideline](https://www.edps.europa.eu/system/files/2024-06/24-06-03_genai_orientations_en.pdf) by the Supervisor
  - [News article](https://www.heise.de/news/ChatGPT-Co-EU-Datenschuetzer-verteidigt-Datenminimierung-9746014.html) by Heise

### AI Guideline by German Data Protection Authority

- from May 2024
- [Guideline](https://www.datenschutzkonferenz-online.de/media/oh/20240506_DSK_Orientierungshilfe_KI_und_Datenschutz.pdf)
- [Additional resolution](https://www.datenschutzkonferenz-online.de/media/dskb/20240503_DSK_Positionspapier_Zustaendigkeiten_KI_VO.pdf)
- [News article 1](https://www.heise.de/news/Datenschutzkonferenz-gibt-Leitfaden-fuer-DSGVO-konforme-KI-Anwendungen-9709228.html) and [article 2](https://www.heise.de/news/AI-Act-Datenschuetzer-wollen-KI-Verordnung-in-Deutschland-durchsetzen-9713089.html) by Heise

### OpenAI Model Specs

- Set of definitions how an LLM should behave in interacting with users
- [Article](https://openai.com/index/introducing-the-model-spec/) by OpenAI
- [News article](https://the-decoder.de/openai-veroeffentlicht-erstmals-richtlinien-fuer-ki-modellverhalten/) by The Decoder

## LLM Security Libraries

[Blog article](https://machine-learning-made-simple.medium.com/7-methods-to-secure-llm-apps-from-prompt-injections-and-jailbreaks-11987b274012) on Medium that introduces multiple libraries

### [Rebuff](https://github.com/protectai/rebuff)

- Prompt injection detection

### [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)

- Add guardrails to conversational agents

### [LangKit](https://github.com/whylabs/langkit)

- Monitor LLMs and prevent prompt attacks

### [LLM Guard](https://github.com/protectai/llm-guard)

- Detect harmful language, prevent data leakage, and prompt injection

### [LVE Repository](https://github.com/lve-org/lve)

- Listing of LLM vulnerabilities

## Certification for AI Models

- In Germany by T√úV Nord ([news article](https://www.heise.de/news/KI-Update-Deep-Dive-Tuev-IT-ueber-KI-Zertifizierung-9706680.html))

## State of BYOAI

- In early 2024, many employees already use AI for their tasks at work __without__ their managers knowing about it ("Bring Your Own AI")
- [Survey](https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/05/2024_Work_Trend_Index_Annual_Report_663d45200a4ad.pdf) by Microsoft and LinkedIn
- [News article](https://the-decoder.de/laut-microsoft-ist-ki-scham-am-arbeitsplatz-eine-sache/) by The Decoder
